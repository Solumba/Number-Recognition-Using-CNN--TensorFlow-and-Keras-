{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import Libraries</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load the MNIST dataset and split into training and test sets</h4>\n",
    "\n",
    "Each pixel in the MNIST dataset has a value between 0 (black) and 255 (white).\n",
    "\n",
    "Neural networks work better when inputs are small numbers, usually between 0 and 1 so divide the pixel values by 255.0 to get numbers representing grey which fall between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Preprocess the data\n",
    "x_train = x_train / 255.0  # Normalize to 0-1\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST images are 28x28 grayscale images.\n",
    "\n",
    "CNNs expect input in this shape: (height, width, channels)\n",
    "\n",
    "Channels: 1 for grayscale, 3 for RGB\n",
    "\n",
    "The -1 lets NumPy automatically figure out the number of samples (e.g., 60 for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape for CNN input (batch_size, height, width, channels)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model Development</h4>\n",
    "\n",
    "The model is sequential, meaning that the layers are stacked one after the other — a straight-through network.\n",
    "Non-linearity is essential as Without non-linearity, the neural network would just be doing a series of linear operations — which means no matter how deep the model is, it could only learn linear patterns. That severely limits what it can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # 10 classes for digits 0-9\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compiling and training the Model</h4>\n",
    "Here, the algorithm to use for optimization is set, what kind of loss to minimize, and what metrics to track.”\n",
    "\n",
    "Adam is a smart optimization algorithm which adjusts learning rates automatically while training. It helps your model learn faster and more reliably than plain gradient descent which makes it the most popular choice for training neural networks.\n",
    "\n",
    "loss='sparse_categorical_crossentropy'\n",
    "This is the loss function (aka cost function) that tells the model how wrong its predictions are and helps it learn from mistakes.\n",
    "\n",
    "sparse_categorical_crossentropy is used for Multi-class classification i.e When the labels are integers (e.g., 0–9 for digits) instead of one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 18s 10ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0408 - val_accuracy: 0.9925\n",
      "Epoch 2/5\n",
      "1685/1688 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9981"
     ]
    }
   ],
   "source": [
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
    "\n",
    "#Save the trained model to a file\n",
    "model.save(\"digit_model.h5\")\n",
    "print(\"Model saved as digit_model.h5\")\n",
    "\n",
    "#Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Step 7: Predict a sample digit\n",
    "index = 1  # pick any digit from test set\n",
    "plt.imshow(x_test[index].reshape(28,28), cmap='gray')\n",
    "plt.title(\"Actual Label: {}\".format(y_test[index]))\n",
    "plt.show()\n",
    "\n",
    "prediction = model.predict(np.expand_dims(x_test[index], axis=0))\n",
    "print(f\"Predicted digit: {np.argmax(prediction)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
